{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92a6c23",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used in machine learning to transform numerical features of a dataset to a common scale. The goal of Min-Max scaling is to scale the values of the features to a specific range, typically between 0 and 1. This can help in improving the performance of machine learning algorithms, especially those that are sensitive to the scale of the input data.\n",
    "\n",
    "The Min-Max scaling formula for a feature \\(x\\) is given by:\n",
    "\n",
    "$ x_{\\text{scaled}} = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} $\n",
    "\n",
    "Where:\n",
    "- x is the original value of the feature.\n",
    "- min(x) is the minimum value of the feature in the dataset.\n",
    "- max(x) is the maximum value of the feature in the dataset.\n",
    "- scaled is the scaled value of the feature within the range [0, 1].\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say we have a dataset of housing prices with two features: \"Square footage\" and \"Number of bedrooms.\" The values for these features are as follows:\n",
    "\n",
    "- Square footage: [1000, 1500, 2000, 1200, 1800]\n",
    "\n",
    "We want to apply Min-Max scaling to these features. First, we calculate the minimum and maximum values for each feature:\n",
    "\n",
    "- Square footage: Min = 1000, Max = 2000\n",
    "\n",
    "Now we can use the Min-Max scaling formula to scale the features:\n",
    "\n",
    "For Square footage = 1200:\n",
    "$ x_{\\text{scaled}} = \\frac{1200 - 1000}{2000 - 1000} = 0.2 $\n",
    "\n",
    "For Number of bedrooms = 3:\n",
    "\n",
    "After scaling, the scaled values for the features are:\n",
    "\n",
    "- Scaled Square footage: [0.0, 0.5, 1.0, 0.2, 0.8]\n",
    "\n",
    "Now both features are within the range [0, 1], which can help algorithms that are sensitive to feature scaling, such as gradient descent-based optimization algorithms, perform more effectively and converge faster.\n",
    "\n",
    "It's important to note that Min-Max scaling assumes that the data follows a uniform distribution within the specified range, and it may not be suitable for features with outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffedf79",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "The Unit Vector technique, also known as Vector Normalization or L2 Normalization, is a feature scaling method used to transform the feature vectors in a dataset to have a unit magnitude. In this technique, each feature vector is divided by its Euclidean norm (L2 norm) to ensure that the resulting vector has a length of 1. This can be particularly useful when working with distance-based algorithms, as it ensures that all feature vectors have equal influence in terms of magnitude.\n",
    "\n",
    "Mathematically, for a feature vector \\(x\\), the unit vector $x_{\\text{normalized}}$ is calculated as:\n",
    "\n",
    "$ x_{\\text{normalized}} = \\frac{x}{\\|x\\|_2} $\n",
    "\n",
    "Where:\n",
    "- \\(x\\) is the original feature vector.\n",
    "- \\(\\|x\\|_2\\) is the Euclidean norm (L2 norm) of the feature vector, calculated as \\(\\sqrt{\\sum_{i=1}^n x_i^2}\\), where \\(n\\) is the number of dimensions in the feature vector.\n",
    "\n",
    "Differences between Unit Vector (L2 Normalization) and Min-Max Scaling:\n",
    "\n",
    "1. **Range of values:**\n",
    "   - Unit Vector: Scales the feature vectors to have a magnitude of 1. The values of the individual features might still be in varying ranges.\n",
    "   - Min-Max Scaling: Scales the feature values within a specific range, typically between 0 and 1.\n",
    "\n",
    "2. **Effect on distribution:**\n",
    "   - Unit Vector: Does not necessarily ensure that the individual feature values are in the same range or follow a specific distribution.\n",
    "   - Min-Max Scaling: Transforms feature values to a uniform range, possibly altering the original distribution.\n",
    "\n",
    "3. **Use cases:**\n",
    "   - Unit Vector: Commonly used when the magnitude of feature vectors is important, such as in distance-based algorithms like k-nearest neighbors or when dealing with text data and cosine similarity.\n",
    "   - Min-Max Scaling: Useful when you want to bring all feature values to a common scale and retain the relative relationships between features. It's also helpful for algorithms that rely on gradients, like gradient descent-based optimization.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset of two-dimensional points representing different data samples. The feature vectors are as follows:\n",
    "\n",
    "- Point A: [3, 4]\n",
    "- Point B: [6, 8]\n",
    "- Point C: [1, 2]\n",
    "\n",
    "We want to apply Unit Vector scaling to these feature vectors. First, we calculate the Euclidean norms of each vector:\n",
    "\n",
    "- Euclidean norm of A: $\\sqrt{3^2 + 4^2} = 5$\n",
    "- Euclidean norm of B: $\\sqrt{6^2 + 8^2} = 10$\n",
    "- Euclidean norm of C: $\\sqrt{1^2 + 2^2} = 2.236$\n",
    "\n",
    "Now we normalize the feature vectors:\n",
    "\n",
    "- Unit Vector of A: $\\left[\\frac{3}{5}, \\frac{4}{5}\\right] = [0.6, 0.8]$\n",
    "- Unit Vector of B: $\\left[\\frac{6}{10}, \\frac{8}{10}\\right] = [0.6, 0.8]$\n",
    "- Unit Vector of C: $\\left[\\frac{1}{2.236}, \\frac{2}{2.236}\\right] = [0.447, 0.894]$\n",
    "\n",
    "As you can see, the magnitude of each feature vector is now 1, while the relative directions between the feature vectors are preserved. This can be useful in scenarios where the magnitude of the vectors is significant, such as when calculating distances or similarities between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe18fe",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining as much of the original variance as possible. It achieves this by identifying the principal components, which are orthogonal (uncorrelated) linear combinations of the original features. The first principal component captures the most significant variance in the data, the second captures the second most significant variance, and so on. PCA is commonly used for data visualization, noise reduction, and speeding up machine learning algorithms by reducing the number of features.\n",
    "\n",
    "The steps involved in performing PCA are as follows:\n",
    "\n",
    "1. Standardize the data: Center the data by subtracting the mean from each feature and scale by dividing by the standard deviation. This ensures that each feature has a similar scale, which is important for PCA.\n",
    "\n",
    "2. Calculate the covariance matrix: Calculate the covariance matrix of the standardized data. The covariance matrix shows how different features vary with respect to each other.\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance (principal components), and eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select principal components: Sort the eigenvectors based on their corresponding eigenvalues in decreasing order. The top \\(k\\) eigenvectors (principal components) where \\(k\\) is the desired dimensionality of the reduced space are selected.\n",
    "\n",
    "5. Project the data: Transform the original data into the reduced-dimensional space by projecting it onto the selected principal components.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset of points in a two-dimensional space. The data is given by:\n",
    "\n",
    "- Point A: [2, 3]\n",
    "- Point B: [5, 5]\n",
    "- Point C: [9, 10]\n",
    "- Point D: [13, 15]\n",
    "- Point E: [16, 18]\n",
    "\n",
    "We want to apply PCA to reduce the dimensionality of this data to one dimension (from 2D to 1D). Here are the steps:\n",
    "\n",
    "1. Standardize the data:\n",
    "\n",
    "Mean of X: (2 + 5 + 9 + 13 + 16) / 5 = 9\n",
    "\n",
    "Mean of Y: (3 + 5 + 10 + 15 + 18) / 5 = 10.2\n",
    "\n",
    "Standard deviation of X: sqrt((1^2 + 4^2 + 0^2 + 4^2 + 7^2) / 5) ≈ 5.48\n",
    "\n",
    "Standard deviation of Y: sqrt((0.2^2 + 0.2^2 + 0.2^2 + 0.2^2 + 0.2^2) / 5) ≈ 0.2\n",
    "\n",
    "\n",
    "Standardized data:\n",
    "- Point A: [(-7/5), (-21/10)]\n",
    "- Point B: [(-2/5), (-21/10)]\n",
    "- Point C: [(7/5), (-16/10)]\n",
    "- Point D: [(17/5), (-5/10)]\n",
    "- Point E: [(24/5), (16/10)]\n",
    "\n",
    "2. Calculate the covariance matrix:\n",
    "\n",
    "Covariance matrix:\n",
    "```\n",
    "[[ 1.0       0.9868152]\n",
    " [ 0.9868152 1.0      ]]\n",
    "```\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues:\n",
    "\n",
    "Eigenvalues: [1.9868152, 0.0131848]\n",
    "Eigenvectors: [0.70710678, 0.70710678], [-0.70710678, 0.70710678]\n",
    "\n",
    "4. Select principal components:\n",
    "\n",
    "Since we want to reduce the dimension to 1, we'll select the eigenvector corresponding to the highest eigenvalue: [0.70710678, 0.70710678]\n",
    "\n",
    "5. Project the data:\n",
    "\n",
    "Projected data onto the first principal component:\n",
    "- Point A: 2 * 0.70710678 + 3 * 0.70710678 ≈ 4.9497\n",
    "- Point B: 5 * 0.70710678 + 5 * 0.70710678 ≈ 7.0711\n",
    "- Point C: 9 * 0.70710678 + 10 * 0.70710678 ≈ 13.4350\n",
    "- Point D: 13 * 0.70710678 + 15 * 0.70710678 ≈ 18.3848\n",
    "- Point E: 16 * 0.70710678 + 18 * 0.70710678 ≈ 24.4439\n",
    "\n",
    "The reduced-dimensional data along the first principal component is:\n",
    "[4.9497, 7.0711, 13.4350, 18.3848, 24.4439]\n",
    "\n",
    "In this example, the original 2D data has been effectively reduced to 1D while retaining much of the original variance. This reduction can help simplify visualization and potentially improve the efficiency of subsequent machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b8865",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "PCA (Principal Component Analysis) is a dimensionality reduction technique that can also be used for feature extraction. While PCA is often associated with dimensionality reduction, it can be thought of as a way to extract new features that capture the most important information from the original features. In essence, PCA transforms the original features into a new set of uncorrelated features (principal components) that are a linear combination of the original features. These new features can serve as a compact representation of the data, effectively extracting the most relevant information.\n",
    "\n",
    "The relationship between PCA and feature extraction can be summarized as follows:\n",
    "\n",
    "1. **Dimensionality Reduction using PCA:** PCA is primarily used to reduce the dimensionality of the data by projecting it onto a lower-dimensional subspace spanned by the principal components. This helps in reducing the computational complexity of algorithms and mitigating the curse of dimensionality.\n",
    "\n",
    "2. **Feature Extraction using PCA:** PCA can also be used as a feature extraction technique. In this context, instead of reducing the dimensionality for the purpose of simplifying computations, the focus is on extracting a new set of features that represent the most important patterns or variances in the data. These new features can then be used for downstream tasks such as classification, clustering, or visualization.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider an image dataset containing grayscale images of handwritten digits (0-9) with pixel values as features. Each image is represented as a vector of pixel values. The original dataset has a high dimensionality, with each pixel acting as a separate feature. We want to use PCA for feature extraction to create a lower-dimensional representation of the images.\n",
    "\n",
    "Steps for Feature Extraction using PCA:\n",
    "\n",
    "1. **Data Preparation:** Flatten each image into a vector to create a matrix where each row represents an image.\n",
    "\n",
    "2. **Standardization:** Standardize the pixel values (features) by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "3. **PCA:** Perform PCA on the standardized data to compute the principal components and corresponding eigenvalues.\n",
    "\n",
    "4. **Select Components:** Choose the top \\(k\\) principal components based on their corresponding eigenvalues. These components capture the most significant variances in the data.\n",
    "\n",
    "5. **Transform Data:** Transform the original standardized data using the selected principal components to obtain the new feature representations.\n",
    "\n",
    "The result is a dataset with a reduced number of new features (principal components) that capture the essential information from the original images.\n",
    "\n",
    "For instance, if we choose to retain the top 50 principal components, we have effectively transformed the original high-dimensional pixel values into a lower-dimensional feature representation that still captures much of the variation in the images. These new features can be used for various tasks like classification or clustering, potentially leading to improved performance compared to using the raw pixel values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d456f2",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be applied to preprocess the numerical features (such as price, rating, and delivery time) before using them in the recommendation algorithm. Min-Max scaling will transform these features into a common scale, typically between 0 and 1, which can help the recommendation algorithm perform effectively and make fair comparisons between different items. Here's how you would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "1. **Understand the Data:** First, you need to understand the range and distribution of each numerical feature in the dataset, such as the minimum and maximum values of price, rating, and delivery time.\n",
    "\n",
    "2. **Apply Min-Max Scaling:** For each feature, you will apply the Min-Max scaling formula to transform the values to a common scale between 0 and 1.\n",
    "\n",
    "   The Min-Max scaling formula for a feature \\(x\\) is given by:\n",
    "   \n",
    "   $ x_{\\text{scaled}} = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} $\n",
    "   \n",
    "   Where:\n",
    "   - \\(x\\) is the original value of the feature.\n",
    "   - min(x) is the minimum value of the feature in the dataset.\n",
    "   - max(x) is the maximum value of the feature in the dataset.\n",
    "   - scaled is the scaled value of the feature within the range [0, 1].\n",
    "\n",
    "3. **Apply Min-Max Scaling to Each Feature:** Calculate the scaled values for each feature using the Min-Max scaling formula. This will ensure that all features are transformed to the same scale.\n",
    "\n",
    "4. **Updated Dataset:** Replace the original feature values with their scaled values in the dataset. Now, the features will be on a common scale suitable for recommendation algorithms.\n",
    "\n",
    "5. **Recommendation Algorithm:** Use the preprocessed data as input to your recommendation algorithm. The scaled features will allow the algorithm to effectively compare and recommend items based on price, rating, and delivery time without being biased by the original scale of the features.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say you have the following data for a few food items:\n",
    "\n",
    "- Price: [10, 20, 15, 25, 30]\n",
    "- Rating: [4.2, 4.8, 3.9, 4.5, 4.0]\n",
    "- Delivery Time (minutes): [40, 55, 30, 50, 45]\n",
    "\n",
    "You want to apply Min-Max scaling to these features. First, calculate the minimum and maximum values for each feature:\n",
    "\n",
    "- Minimum Price: 10, Maximum Price: 30\n",
    "- Minimum Rating: 3.9, Maximum Rating: 4.8\n",
    "- Minimum Delivery Time: 30, Maximum Delivery Time: 55\n",
    "\n",
    "Now, apply Min-Max scaling to each feature:\n",
    "\n",
    "- Scaled Price: $\\left[\\frac{10-10}{30-10}, \\frac{20-10}{30-10}, \\frac{15-10}{30-10}, \\frac{25-10}{30-10}, \\frac{30-10}{30-10}\\right] = [0.0, 0.333, 0.167, 0.667, 1.0]$\n",
    "- Scaled Rating: $\\left[\\frac{4.2-3.9}{4.8-3.9}, \\frac{4.8-3.9}{4.8-3.9}, \\frac{3.9-3.9}{4.8-3.9}, \\frac{4.5-3.9}{4.8-3.9}, \\frac{4.0-3.9}{4.8-3.9}\\right] = [0.6, 1.0, 0.0, 0.8, 0.2]$\n",
    "- Scaled Delivery Time: $\\left[\\frac{40-30}{55-30}, \\frac{55-30}{55-30}, \\frac{30-30}{55-30}, \\frac{50-30}{55-30}, \\frac{45-30}{55-30}\\right] = [0.363, 1.0, 0.0, 0.727, 0.545]$\n",
    "\n",
    "Now, your dataset with scaled features can be used as input to your recommendation algorithm. The algorithm will work with these scaled values, allowing it to effectively compare and recommend food items based on price, rating, and delivery time while considering the appropriate scale for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca966e5",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "When working on a project to predict stock prices with a dataset that contains many features, such as company financial data and market trends, PCA (Principal Component Analysis) can be a valuable technique to reduce the dimensionality of the dataset. By applying PCA, you can transform the original high-dimensional feature space into a lower-dimensional space while retaining as much of the important information and variability as possible. This can lead to improved model performance, reduced computational complexity, and alleviation of the curse of dimensionality. Here's how you would use PCA to achieve dimensionality reduction for your stock price prediction project:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Collect and preprocess your dataset, ensuring that it is clean and properly formatted.\n",
    "   - Standardize the features: It's important to standardize the features by subtracting the mean and dividing by the standard deviation. PCA is sensitive to the scale of features, so standardization helps ensure that all features have equal influence.\n",
    "\n",
    "2. **Covariance Matrix Calculation:**\n",
    "   - Calculate the covariance matrix of the standardized data. The covariance matrix shows how different features vary with respect to each other.\n",
    "\n",
    "3. **Compute Eigenvectors and Eigenvalues:**\n",
    "   - Calculate the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance (principal components), and eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. **Select Principal Components:**\n",
    "   - Sort the eigenvectors based on their corresponding eigenvalues in decreasing order. You can choose the top \\(k\\) eigenvectors (principal components) that collectively capture a significant amount of the total variance in the data. The number of principal components \\(k\\) is a hyperparameter you can tune.\n",
    "\n",
    "5. **Project Data onto Reduced-Dimensional Space:**\n",
    "   - Transform the original standardized data into the reduced-dimensional space by projecting it onto the selected principal components. This transformation creates a new dataset with reduced dimensions.\n",
    "\n",
    "6. **Modeling and Prediction:**\n",
    "   - Train your stock price prediction model using the reduced-dimensional dataset obtained after PCA. You can use various regression techniques or machine learning algorithms for this task.\n",
    "\n",
    "Benefits of Using PCA for Dimensionality Reduction in Stock Price Prediction:\n",
    "\n",
    "1. **Reduced Noise:** PCA can help remove noise and less important features, focusing on the features that explain the most significant variability in the data.\n",
    "\n",
    "2. **Model Complexity:** By reducing the number of features, your prediction model's complexity and computation time can be reduced, leading to faster training and prediction.\n",
    "\n",
    "3. **Visualization:** Lower-dimensional data is easier to visualize, enabling better understanding of relationships between features and potentially revealing patterns.\n",
    "\n",
    "4. **Mitigating Overfitting:** Reducing dimensionality can help mitigate the risk of overfitting, as the model has fewer parameters to fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a877f",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e3442b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst=[1,5,10,15,20]\n",
    "mini=min(lst)\n",
    "maxi=max(lst)\n",
    "\n",
    "c,d=-1,1\n",
    "\n",
    "scaled_lst=[]\n",
    "for i in lst:\n",
    "    scaled_lst.append((((i-mini)*(d-c))/(maxi-mini))+c)\n",
    "scaled_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4c95b",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "To perform feature extraction using PCA on the given dataset with features [height, weight, age, gender, blood pressure], we need to follow these steps:\n",
    "\n",
    "1. **Data Preprocessing:** Standardize the features (height, weight, age, and blood pressure) by subtracting the mean and dividing by the standard deviation. Categorical features like \"gender\" might need to be encoded as numerical values (e.g., 0 for male, 1 for female) to make them suitable for PCA.\n",
    "\n",
    "2. **Covariance Matrix:** Calculate the covariance matrix of the standardized features.\n",
    "\n",
    "3. **Eigenvectors and Eigenvalues:** Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4. **Select Principal Components:** Sort the eigenvectors based on their corresponding eigenvalues in decreasing order. Decide on the number of principal components to retain based on the amount of variance explained and the desired dimensionality reduction. This is a crucial step that requires careful consideration.\n",
    "\n",
    "5. **Project Data:** Transform the original standardized data into the reduced-dimensional space using the selected principal components.\n",
    "\n",
    "Choosing the number of principal components to retain depends on the trade-off between preserving information and reducing dimensionality. One common approach is to analyze the cumulative explained variance ratio. The cumulative explained variance ratio shows how much of the total variance is captured by each principal component, sorted in decreasing order. By summing up these ratios, you can decide how many principal components are needed to retain a significant portion of the variance.\n",
    "\n",
    "For example, if you find that the first few principal components capture a high cumulative explained variance (e.g., 95% or more), you might choose to retain those components. Retaining a high proportion of the variance means that the transformed features will still capture most of the important information from the original data.\n",
    "\n",
    "However, the choice of the number of principal components is often domain-specific and can depend on factors such as the application, desired reduction in dimensionality, and the specific characteristics of the dataset. You might also consider the computational efficiency and interpretability of the reduced features.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
