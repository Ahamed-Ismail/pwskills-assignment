{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6bfa51a",
   "metadata": {},
   "source": [
    "# Part 1: Understanding Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6bc8a",
   "metadata": {},
   "source": [
    "## Q1. What is the role of optimization algorithms in artificial neural networks? Why are they necessary?\n",
    "\n",
    "The role of optimization algorithms in artificial neural networks is to minimize the cost or loss function by adjusting the weights and biases during the training process. They are necessary to help the network converge to an optimal set of parameters, thereby improving its ability to make accurate predictions on new data.\n",
    "\n",
    "\n",
    "## Q2. Explain the concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms of convergence speed and memory requirements.\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the loss function by adjusting the model parameters iteratively. Its variants include:\n",
    "   - Stochastic Gradient Descent (SGD): Uses a randomly selected subset of data for each iteration.\n",
    "   - Mini-batch Gradient Descent: Computes the gradient using small batches of data. \n",
    "   Differences and tradeoffs among these variants primarily involve convergence speed, where SGD is faster but noisy, and mini-batch strikes a balance. Memory requirements are higher for batch gradient descent but comparatively lower for the other variants.\n",
    "\n",
    "\n",
    "## Q3. Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow convergence, local minima). How do modern optimizers address these challenges?\n",
    "\n",
    "Traditional gradient descent optimization methods often face challenges such as slow convergence and the risk of getting trapped in local minima. Modern optimizers address these issues by incorporating techniques like momentum, adaptive learning rates, and learning rate schedules. These methods help in faster convergence and enable the optimization process to avoid getting stuck in local minima.\n",
    "\n",
    "\n",
    "## Q4. Discuss the concepts of momentum and learning rate in the context of optimization algorithms .How do they impact convergence and model performance\n",
    "\n",
    "Momentum in optimization algorithms helps accelerate SGD in the relevant direction and dampen oscillations. It accumulates a velocity vector in directions of persistent reduction in the loss, enabling faster convergence. Learning rate, on the other hand, controls the step size during the optimization process. A higher learning rate can lead to faster convergence but may cause oscillations, while a lower learning rate can lead to slow convergence. Finding an appropriate balance between momentum and learning rate is crucial for optimizing convergence and improving overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797459bf",
   "metadata": {},
   "source": [
    "# Part 2: Optimizer Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd4bfb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Q1. Explain the concept of Stochastic Gradient Descent (SGD) and its advantages compared to traditional gradient descent. Discuss its limitations and scenarios where it is most suitable.\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is an optimization algorithm that randomly selects a subset of the training data for each iteration, making it faster than traditional gradient descent. Its advantages include faster convergence and the ability to handle large datasets efficiently. However, it may introduce noise due to the random nature of the selection and may require fine-tuning of the learning rate. It is most suitable in scenarios where computational resources are limited, and the dataset is large.\n",
    "\n",
    "## Q2. Describe the concept of Adam optimizer and how it combines momentum and adaptive learning rates. Discuss its benefits and potential drawbacks.\n",
    "\n",
    "The Adam optimizer combines the concepts of momentum and adaptive learning rates. It uses the moving averages of both the gradient and its square to scale the learning rate for each parameter. The benefits of Adam include fast convergence, good performance on sparse gradients, and the ability to handle non-stationary objectives. However, it may require careful tuning of hyperparameters and can converge to sharp minima, leading to overfitting in some cases.\n",
    "\n",
    "\n",
    "##  Q3. Explain the concept of RMSprop optimizer and how it addresses the challenges of adaptive learning rates. Compare it with Adam and discuss their relative strengths and weaknesses.\n",
    "\n",
    "RMSprop is an optimizer that addresses the challenges of adaptive learning rates by using a moving average of squared gradients to normalize the learning rate. It divides the learning rate for each parameter by the square root of the mean of the previous gradients. Compared to Adam, RMSprop has lower memory requirements and is typically easier to tune. However, it may converge more slowly and is sensitive to the choice of the initial learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fa525",
   "metadata": {},
   "source": [
    "## 1. Implement SD, Adam, and RMSprop optimizers in a deep learning model using a framework of your choice. Train the model on a suitable dataset and compare their impact on model convergence and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebb62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "LAYERS=[\n",
    "    layers.Flatten(input_shape=[28,28]),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "]\n",
    "\n",
    "model1 = models.Sequential(LAYERS)\n",
    "model2 = models.Sequential(LAYERS)\n",
    "model3 = models.Sequential(LAYERS)\n",
    "\n",
    "\n",
    "model1.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # Stochastic Gradient Descent\n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # Adam\n",
    "model3.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # RMSprop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af80b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist=tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032ea6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain_full,ytrain_full),(xtest,ytest)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6252dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvalid,xtrain= xtrain_full[:5000]/255. , xtrain_full[5000:]/255.\n",
    "yvalid,ytrain= ytrain_full[:5000] , ytrain_full[5000:]\n",
    "\n",
    "xtest=xtest/255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5e0cf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2650 - accuracy: 0.9230 - val_loss: 0.2301 - val_accuracy: 0.9346\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2304 - accuracy: 0.9339 - val_loss: 0.2037 - val_accuracy: 0.9414\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2052 - accuracy: 0.9403 - val_loss: 0.1829 - val_accuracy: 0.9468\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1855 - accuracy: 0.9462 - val_loss: 0.1667 - val_accuracy: 0.9530\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1704 - accuracy: 0.9504 - val_loss: 0.1575 - val_accuracy: 0.9542\n"
     ]
    }
   ],
   "source": [
    "history1=model1.fit(xtrain, ytrain, epochs=5, batch_size=32, validation_data=(xvalid,yvalid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd994b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1647 - accuracy: 0.9510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16474656760692596, 0.9509999752044678]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f5754a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 9s 4ms/step - loss: 0.1628 - accuracy: 0.9514 - val_loss: 0.1276 - val_accuracy: 0.9620\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1053 - accuracy: 0.9687 - val_loss: 0.1014 - val_accuracy: 0.9674\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0825 - accuracy: 0.9735 - val_loss: 0.0918 - val_accuracy: 0.9694\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.0658 - accuracy: 0.9792 - val_loss: 0.0940 - val_accuracy: 0.9702\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0542 - accuracy: 0.9823 - val_loss: 0.1167 - val_accuracy: 0.9644\n"
     ]
    }
   ],
   "source": [
    "history2=model2.fit(xtrain, ytrain, epochs=5, batch_size=32, validation_data=(xvalid,yvalid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04ee98f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.1042 - accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10419125854969025, 0.9771000146865845]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf9d4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0382 - accuracy: 0.9876 - val_loss: 0.0800 - val_accuracy: 0.9784\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0334 - accuracy: 0.9895 - val_loss: 0.0885 - val_accuracy: 0.9800\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0310 - accuracy: 0.9904 - val_loss: 0.0941 - val_accuracy: 0.9786\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0270 - accuracy: 0.9919 - val_loss: 0.1155 - val_accuracy: 0.9748\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0242 - accuracy: 0.9925 - val_loss: 0.1185 - val_accuracy: 0.9748\n"
     ]
    }
   ],
   "source": [
    "history3=model3.fit(xtrain, ytrain, epochs=5, batch_size=32, validation_data=(xvalid,yvalid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af270673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1042 - accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10419125854969025, 0.9771000146865845]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066074c",
   "metadata": {},
   "source": [
    "# Part 3: Applying optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58334e1b",
   "metadata": {},
   "source": [
    "## 2. Discuss the considerations and tradeoffs when choosing the appropriate optimizer for a given neural network architecture and task. onsider factors such as convergence speed, stability, and generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d8710",
   "metadata": {},
   "source": [
    "When selecting an optimizer for a specific neural network architecture and task, several considerations and tradeoffs need to be evaluated. The choice of optimizer can significantly impact the performance and efficiency of the training process. Here are some key considerations and tradeoffs to keep in mind:\n",
    "\n",
    "1. **Convergence Speed**: Different optimizers have varying convergence speeds. Optimizers like Adam and RMSprop often converge faster compared to standard Stochastic Gradient Descent (SGD), especially when dealing with complex and high-dimensional data. However, faster convergence may come at the cost of increased computational complexity.\n",
    "\n",
    "2. **Stability**: Some optimizers are more stable than others. For instance, SGD can exhibit more oscillations during training, while optimizers like RMSprop and Adam tend to provide smoother convergence. However, the choice of the learning rate can also impact the stability of the optimization process.\n",
    "\n",
    "3. **Generalization Performance**: While certain optimizers may enable the model to converge quickly during training, they may not necessarily lead to better generalization performance on unseen data. It is essential to consider the balance between optimization speed and the ability of the model to generalize well to new, unseen data.\n",
    "\n",
    "4. **Sensitivity to Learning Rate and Hyperparameters**: Different optimizers have varying sensitivity to learning rate and other hyperparameters. For instance, Adam may require more careful tuning of hyperparameters compared to SGD or RMSprop. Sensitivity to hyperparameters can impact the stability and convergence of the optimization process.\n",
    "\n",
    "5. **Memory and Computational Requirements**: Optimizers like Adam tend to have higher memory requirements due to the need to maintain additional parameters such as the moving averages of the gradients. This can be a consideration when working with limited computational resources or when training large models.\n",
    "\n",
    "6. **Robustness to Noisy Data or Sparse Gradients**: Some optimizers, such as Adam, exhibit robustness to noisy data and sparse gradients, enabling more efficient training in such scenarios. Understanding the characteristics of the dataset and the network architecture is crucial for choosing an appropriate optimizer.\n",
    "\n",
    "7. **Complexity of the Optimization Landscape**: Optimizers can behave differently based on the complexity of the optimization landscape. In cases where the optimization landscape is complex with many local minima, adaptive methods like Adam may perform better compared to simple optimizers like SGD.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
