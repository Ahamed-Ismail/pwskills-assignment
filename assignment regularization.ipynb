{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b681058",
   "metadata": {},
   "source": [
    "# Part 1: Understanding Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada7eeb",
   "metadata": {},
   "source": [
    "## 1. What is regularization in the context of deep learning? Why is it important?\n",
    "\n",
    "\n",
    "Regularization in the context of deep learning refers to a set of techniques used to prevent a model from overfitting to the training data. It involves introducing additional information, such as a penalty term on the model's complexity, to the objective function being optimized during training. Regularization is important because it helps control the model's capacity and prevents it from becoming overly complex, thereby improving its generalization to unseen data.\n",
    "\n",
    "\n",
    "## 2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff.\n",
    "\n",
    "The bias-variance tradeoff is a key concept in machine learning that highlights the tradeoff between a model's ability to fit the training data closely (low bias) and its sensitivity to variations in the training data (high variance). High bias can lead to underfitting, while high variance can lead to overfitting. Regularization helps in addressing this tradeoff by reducing model complexity, which decreases variance but might slightly increase bias.\n",
    "\n",
    "## 3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the model?\n",
    "\n",
    "L1 and L2 regularization are two common regularization techniques. L1 regularization adds the sum of the absolute values of the coefficients to the loss function, while L2 regularization adds the sum of the squares of the coefficients. L1 regularization encourages sparsity and feature selection, as it tends to force some coefficients to be exactly zero. L2 regularization, on the other hand, allows small weights but doesn't force them to be exactly zero. The effects of L1 and L2 regularization include different types of weight shrinkage and sparsity-inducing effects on the model's parameters.\n",
    "\n",
    "## 4. Discuss the role of regularization in preventing overfitting and improving the generalization of deep learning models.\n",
    "\n",
    "Regularization plays a crucial role in preventing overfitting by constraining the model's capacity and preventing it from fitting the noise in the training data. By doing so, it encourages the learning of essential patterns and relationships, ultimately improving the model's ability to generalize to unseen data. Regularization techniques thus help create models that perform well on both the training data and new, unseen data, enhancing the overall performance and robustness of deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04074944",
   "metadata": {},
   "source": [
    "# Part 2: Regularization Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76025d04",
   "metadata": {},
   "source": [
    "## 1. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on model training and inference.\n",
    "\n",
    "Dropout regularization is a technique used to reduce overfitting in neural networks. During training, it randomly sets a fraction of the nodes in a layer to zero, effectively 'dropping out' those nodes along with all of their connections. This prevents the network from relying too much on certain nodes and helps it learn more robust features. During inference, the full network is used, but the weights of the nodes are scaled to account for the dropout probability used during training. Dropout has the effect of preventing complex co-adaptations on training data, thus reducing overfitting. It also implicitly creates a large number of different neural network architectures, forcing the network to learn more robust features.\n",
    "\n",
    "## 2. Describe the concept of Early stopping as a form of regularization. How does it help prevent overfitting during the training process?\n",
    "\n",
    "Early stopping is a regularization technique used to prevent overfitting during the training process. It involves monitoring the performance of the model on a separate validation dataset and stopping the training process once the performance on the validation dataset starts to degrade. By stopping the training at an optimal point, early stopping prevents the model from continuing to learn the noise present in the training data, thereby enhancing the generalization ability of the model.\n",
    "\n",
    "## 3. Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch Normalization help in preventing overfitting?\n",
    "\n",
    "Batch Normalization is a technique used to normalize the inputs of each layer to have a mean of zero and a variance of one. It helps in stabilizing the learning process and allows for higher learning rates. This normalization has the regularization effect of reducing internal covariate shift, which in turn reduces the need for regularization techniques like dropout. By reducing the internal covariate shift and ensuring that the inputs to each layer are normalized, Batch Normalization helps prevent overfitting by making the optimization process more stable and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0aedf2",
   "metadata": {},
   "source": [
    "# Part 3: Applying Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff2019",
   "metadata": {},
   "source": [
    "## 1. Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate its impact on model performance and compare it with a model without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "250277aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b66a16c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol quality  \n",
       "0      9.4     bad  \n",
       "1      9.8     bad  \n",
       "2      9.8     bad  \n",
       "3      9.8    good  \n",
       "4      9.4     bad  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(r'C:\\Users\\tanji\\Desktop\\myPW\\assignments\\datasets\\wine.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9cf3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder=OrdinalEncoder()\n",
    "\n",
    "df['quality']=encoder.fit_transform(df[['quality']])\n",
    "df['quality']=df['quality'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "652f122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop('quality', axis=1)\n",
    "y=df['quality']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain,xtest,ytrain,ytest= train_test_split(x,y,test_size=0.25, random_state=42)\n",
    "\n",
    "xtrain,xvalid,ytrain,yvalid=train_test_split(x,y,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c48c0d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()\n",
    "xtrain=scaler.fit_transform(xtrain)\n",
    "xtest=scaler.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f19b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "274838e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS1=[\n",
    "    tf.keras.layers.Flatten(input_shape=xtrain.shape[1:]),\n",
    "    tf.keras.layers.Dense(300,activation='relu'),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "]\n",
    "\n",
    "model1=tf.keras.Sequential(LAYERS1)\n",
    "\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e079123",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.5500 - accuracy: 0.7264 - val_loss: 13.0631 - val_accuracy: 0.4400\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.5097 - accuracy: 0.7623 - val_loss: 5.7375 - val_accuracy: 0.4725\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4945 - accuracy: 0.7640 - val_loss: 6.3973 - val_accuracy: 0.4875\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4832 - accuracy: 0.7706 - val_loss: 7.2803 - val_accuracy: 0.4725\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4740 - accuracy: 0.7848 - val_loss: 6.7225 - val_accuracy: 0.4725\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4770 - accuracy: 0.7756 - val_loss: 11.2175 - val_accuracy: 0.4425\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4567 - accuracy: 0.7915 - val_loss: 10.7725 - val_accuracy: 0.4800\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4515 - accuracy: 0.7848 - val_loss: 19.5570 - val_accuracy: 0.4625\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4480 - accuracy: 0.7907 - val_loss: 14.3465 - val_accuracy: 0.4400\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.7965 - val_loss: 12.4116 - val_accuracy: 0.4525\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4271 - accuracy: 0.8032 - val_loss: 20.0775 - val_accuracy: 0.4625\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4205 - accuracy: 0.8098 - val_loss: 16.4673 - val_accuracy: 0.4475\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4101 - accuracy: 0.8098 - val_loss: 21.1214 - val_accuracy: 0.4625\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3977 - accuracy: 0.8198 - val_loss: 13.9495 - val_accuracy: 0.4675\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3961 - accuracy: 0.8140 - val_loss: 19.2672 - val_accuracy: 0.4375\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3857 - accuracy: 0.8173 - val_loss: 18.7768 - val_accuracy: 0.4450\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3742 - accuracy: 0.8265 - val_loss: 23.3309 - val_accuracy: 0.4525\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3637 - accuracy: 0.8340 - val_loss: 26.1181 - val_accuracy: 0.4575\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3577 - accuracy: 0.8382 - val_loss: 25.3427 - val_accuracy: 0.4425\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3434 - accuracy: 0.8449 - val_loss: 28.1624 - val_accuracy: 0.4350\n"
     ]
    }
   ],
   "source": [
    "histor1=model1.fit(xtrain,ytrain, validation_data=(xvalid,yvalid), batch_size=10, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e593250",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS2=[\n",
    "    tf.keras.layers.Flatten(input_shape=xtrain.shape[1:]),\n",
    "    tf.keras.layers.Dense(300,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "]\n",
    "\n",
    "model2=tf.keras.Sequential(LAYERS2)\n",
    "\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23263911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.5665 - accuracy: 0.7073 - val_loss: 8.3713 - val_accuracy: 0.4550\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.7556 - val_loss: 8.9948 - val_accuracy: 0.4450\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.5072 - accuracy: 0.7640 - val_loss: 7.4747 - val_accuracy: 0.4525\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4978 - accuracy: 0.7640 - val_loss: 8.5108 - val_accuracy: 0.4675\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4860 - accuracy: 0.7681 - val_loss: 7.7445 - val_accuracy: 0.4800\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4920 - accuracy: 0.7640 - val_loss: 12.3164 - val_accuracy: 0.4450\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4725 - accuracy: 0.7731 - val_loss: 13.8896 - val_accuracy: 0.4425\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4784 - accuracy: 0.7773 - val_loss: 10.6908 - val_accuracy: 0.4525\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4644 - accuracy: 0.7823 - val_loss: 14.6841 - val_accuracy: 0.4450\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4669 - accuracy: 0.7873 - val_loss: 10.9135 - val_accuracy: 0.4775\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.4604 - accuracy: 0.7857 - val_loss: 19.2748 - val_accuracy: 0.4375\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4554 - accuracy: 0.7823 - val_loss: 14.8422 - val_accuracy: 0.4450\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4455 - accuracy: 0.8032 - val_loss: 12.9385 - val_accuracy: 0.4700\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4475 - accuracy: 0.7940 - val_loss: 17.6430 - val_accuracy: 0.4425\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.8040 - val_loss: 12.8987 - val_accuracy: 0.4675\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4357 - accuracy: 0.7973 - val_loss: 13.8696 - val_accuracy: 0.4675\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4223 - accuracy: 0.8115 - val_loss: 14.0071 - val_accuracy: 0.4575\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4186 - accuracy: 0.8182 - val_loss: 18.2808 - val_accuracy: 0.4500\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.7957 - val_loss: 17.1082 - val_accuracy: 0.4475\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4183 - accuracy: 0.8148 - val_loss: 20.3727 - val_accuracy: 0.4475\n"
     ]
    }
   ],
   "source": [
    "history2=model2.fit(xtrain,ytrain, validation_data=(xvalid,yvalid), batch_size=10, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e54baa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.549957</td>\n",
       "      <td>0.726439</td>\n",
       "      <td>13.063059</td>\n",
       "      <td>0.4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.509692</td>\n",
       "      <td>0.762302</td>\n",
       "      <td>5.737496</td>\n",
       "      <td>0.4725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.494537</td>\n",
       "      <td>0.763970</td>\n",
       "      <td>6.397284</td>\n",
       "      <td>0.4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.483184</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>7.280281</td>\n",
       "      <td>0.4725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.474009</td>\n",
       "      <td>0.784821</td>\n",
       "      <td>6.722546</td>\n",
       "      <td>0.4725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.476965</td>\n",
       "      <td>0.775646</td>\n",
       "      <td>11.217520</td>\n",
       "      <td>0.4425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.456748</td>\n",
       "      <td>0.791493</td>\n",
       "      <td>10.772461</td>\n",
       "      <td>0.4800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.451465</td>\n",
       "      <td>0.784821</td>\n",
       "      <td>19.557005</td>\n",
       "      <td>0.4625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.448004</td>\n",
       "      <td>0.790659</td>\n",
       "      <td>14.346549</td>\n",
       "      <td>0.4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.435876</td>\n",
       "      <td>0.796497</td>\n",
       "      <td>12.411610</td>\n",
       "      <td>0.4525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.427069</td>\n",
       "      <td>0.803169</td>\n",
       "      <td>20.077456</td>\n",
       "      <td>0.4625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.420517</td>\n",
       "      <td>0.809842</td>\n",
       "      <td>16.467327</td>\n",
       "      <td>0.4475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.410053</td>\n",
       "      <td>0.809842</td>\n",
       "      <td>21.121367</td>\n",
       "      <td>0.4625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.397679</td>\n",
       "      <td>0.819850</td>\n",
       "      <td>13.949477</td>\n",
       "      <td>0.4675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.396132</td>\n",
       "      <td>0.814012</td>\n",
       "      <td>19.267210</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.385740</td>\n",
       "      <td>0.817348</td>\n",
       "      <td>18.776823</td>\n",
       "      <td>0.4450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.374233</td>\n",
       "      <td>0.826522</td>\n",
       "      <td>23.330860</td>\n",
       "      <td>0.4525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.363720</td>\n",
       "      <td>0.834028</td>\n",
       "      <td>26.118120</td>\n",
       "      <td>0.4575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.357717</td>\n",
       "      <td>0.838198</td>\n",
       "      <td>25.342667</td>\n",
       "      <td>0.4425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.844871</td>\n",
       "      <td>28.162409</td>\n",
       "      <td>0.4350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy   val_loss  val_accuracy\n",
       "0   0.549957  0.726439  13.063059        0.4400\n",
       "1   0.509692  0.762302   5.737496        0.4725\n",
       "2   0.494537  0.763970   6.397284        0.4875\n",
       "3   0.483184  0.770642   7.280281        0.4725\n",
       "4   0.474009  0.784821   6.722546        0.4725\n",
       "5   0.476965  0.775646  11.217520        0.4425\n",
       "6   0.456748  0.791493  10.772461        0.4800\n",
       "7   0.451465  0.784821  19.557005        0.4625\n",
       "8   0.448004  0.790659  14.346549        0.4400\n",
       "9   0.435876  0.796497  12.411610        0.4525\n",
       "10  0.427069  0.803169  20.077456        0.4625\n",
       "11  0.420517  0.809842  16.467327        0.4475\n",
       "12  0.410053  0.809842  21.121367        0.4625\n",
       "13  0.397679  0.819850  13.949477        0.4675\n",
       "14  0.396132  0.814012  19.267210        0.4375\n",
       "15  0.385740  0.817348  18.776823        0.4450\n",
       "16  0.374233  0.826522  23.330860        0.4525\n",
       "17  0.363720  0.834028  26.118120        0.4575\n",
       "18  0.357717  0.838198  25.342667        0.4425\n",
       "19  0.343373  0.844871  28.162409        0.4350"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(histor1.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a268c0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.566505</td>\n",
       "      <td>0.707256</td>\n",
       "      <td>8.371330</td>\n",
       "      <td>0.4550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.507300</td>\n",
       "      <td>0.755630</td>\n",
       "      <td>8.994847</td>\n",
       "      <td>0.4450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.507238</td>\n",
       "      <td>0.763970</td>\n",
       "      <td>7.474709</td>\n",
       "      <td>0.4525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.497843</td>\n",
       "      <td>0.763970</td>\n",
       "      <td>8.510765</td>\n",
       "      <td>0.4675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.486001</td>\n",
       "      <td>0.768140</td>\n",
       "      <td>7.744461</td>\n",
       "      <td>0.4800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.491963</td>\n",
       "      <td>0.763970</td>\n",
       "      <td>12.316405</td>\n",
       "      <td>0.4450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.472548</td>\n",
       "      <td>0.773144</td>\n",
       "      <td>13.889614</td>\n",
       "      <td>0.4425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.478407</td>\n",
       "      <td>0.777314</td>\n",
       "      <td>10.690821</td>\n",
       "      <td>0.4525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.464399</td>\n",
       "      <td>0.782319</td>\n",
       "      <td>14.684079</td>\n",
       "      <td>0.4450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.466882</td>\n",
       "      <td>0.787323</td>\n",
       "      <td>10.913460</td>\n",
       "      <td>0.4775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.460403</td>\n",
       "      <td>0.785655</td>\n",
       "      <td>19.274828</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.455425</td>\n",
       "      <td>0.782319</td>\n",
       "      <td>14.842212</td>\n",
       "      <td>0.4450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.445469</td>\n",
       "      <td>0.803169</td>\n",
       "      <td>12.938481</td>\n",
       "      <td>0.4700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.447495</td>\n",
       "      <td>0.793995</td>\n",
       "      <td>17.643007</td>\n",
       "      <td>0.4425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.435537</td>\n",
       "      <td>0.804003</td>\n",
       "      <td>12.898708</td>\n",
       "      <td>0.4675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.435725</td>\n",
       "      <td>0.797331</td>\n",
       "      <td>13.869551</td>\n",
       "      <td>0.4675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.422325</td>\n",
       "      <td>0.811510</td>\n",
       "      <td>14.007142</td>\n",
       "      <td>0.4575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.418627</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>18.280844</td>\n",
       "      <td>0.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.430403</td>\n",
       "      <td>0.795663</td>\n",
       "      <td>17.108223</td>\n",
       "      <td>0.4475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.418273</td>\n",
       "      <td>0.814846</td>\n",
       "      <td>20.372738</td>\n",
       "      <td>0.4475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy   val_loss  val_accuracy\n",
       "0   0.566505  0.707256   8.371330        0.4550\n",
       "1   0.507300  0.755630   8.994847        0.4450\n",
       "2   0.507238  0.763970   7.474709        0.4525\n",
       "3   0.497843  0.763970   8.510765        0.4675\n",
       "4   0.486001  0.768140   7.744461        0.4800\n",
       "5   0.491963  0.763970  12.316405        0.4450\n",
       "6   0.472548  0.773144  13.889614        0.4425\n",
       "7   0.478407  0.777314  10.690821        0.4525\n",
       "8   0.464399  0.782319  14.684079        0.4450\n",
       "9   0.466882  0.787323  10.913460        0.4775\n",
       "10  0.460403  0.785655  19.274828        0.4375\n",
       "11  0.455425  0.782319  14.842212        0.4450\n",
       "12  0.445469  0.803169  12.938481        0.4700\n",
       "13  0.447495  0.793995  17.643007        0.4425\n",
       "14  0.435537  0.804003  12.898708        0.4675\n",
       "15  0.435725  0.797331  13.869551        0.4675\n",
       "16  0.422325  0.811510  14.007142        0.4575\n",
       "17  0.418627  0.818182  18.280844        0.4500\n",
       "18  0.430403  0.795663  17.108223        0.4475\n",
       "19  0.418273  0.814846  20.372738        0.4475"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history2.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4343c",
   "metadata": {},
   "source": [
    "- we could see that model withou dropout has more accuracy but less validation accuracy than model with dropout layers.\n",
    "\n",
    "- Thus we can conclude that adding dropout layers gerneralizes the model and prevents overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83386db9",
   "metadata": {},
   "source": [
    "## 2. Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for acgiven deep learning task.\n",
    "\n",
    "When selecting an appropriate regularization technique for a deep learning task, several considerations and tradeoffs need to be taken into account:\n",
    "\n",
    "1. **Data Complexity**: Consider the complexity and nature of the dataset. If the dataset is large and complex, techniques like Dropout and Batch Normalization might be more suitable. For simpler datasets, simpler forms of regularization like L1 or L2 regularization might suffice.\n",
    "\n",
    "2. **Model Complexity**: The complexity of the deep learning model itself should be considered. More complex models might require stronger regularization techniques to prevent overfitting, while simpler models might not need as much regularization.\n",
    "\n",
    "3. **Computational Resources**: Some regularization techniques, such as Dropout, can increase training time due to the random dropping of nodes. Consider the available computational resources and the time constraints for training the model.\n",
    "\n",
    "4. **Interpretability**: Techniques like L1 regularization can lead to sparse models, making them more interpretable. If interpretability is a key requirement, L1 regularization might be preferred over other techniques.\n",
    "\n",
    "5. **Performance Metrics**: Different regularization techniques might affect the model's performance metrics differently. Consider the impact of the chosen regularization technique on various performance metrics such as accuracy, precision, recall, or F1 score.\n",
    "\n",
    "6. **Generalization vs. Fit to Training Data**: Regularization techniques aim to strike a balance between fitting the training data well and generalizing to unseen data. Assess the tradeoff between model performance on the training data and its ability to generalize to new, unseen data.\n",
    "\n",
    "7. **Overhead and Hyperparameters**: Different regularization techniques come with their own set of hyperparameters that need to be tuned. Consider the overhead of tuning these hyperparameters and the impact of their values on the overall performance of the model.\n",
    "\n",
    "8. **Domain Knowledge and Prior Information**: Consider any prior knowledge or information about the data or the problem domain that might guide the choice of a suitable regularization technique. This could help in selecting the technique that aligns with the underlying characteristics of the data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
