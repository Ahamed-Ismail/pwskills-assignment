{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbaef5c",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "\n",
    "**R-squared (R²)** is a statistical measure that is used to evaluate the goodness of fit of a linear regression model. It provides information about how well the independent variable(s) in your model explain the variation in the dependent variable. In other words, it gives you an idea of how closely the observed data points cluster around the regression line.\n",
    "\n",
    "Mathematically, R² is calculated as follows:\n",
    "\n",
    "$ R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}} $\n",
    "\n",
    "Where:\n",
    "- $ SS_{\\text{res}} $ is the sum of squared residuals, which measures the sum of the squared differences between the actual values and the predicted values.\n",
    "- $ SS_{\\text{tot}} $ is the total sum of squares, which measures the sum of the squared differences between the actual values and the mean of the dependent variable.\n",
    "\n",
    "R² values range from 0 to 1. Here's what different R² values indicate:\n",
    "\n",
    "- **R² = 0**: The model does not explain any of the variability in the dependent variable. It's essentially as good as using the mean of the dependent variable as a predictor.\n",
    "- **0 < R² < 1**: The model explains a portion of the variability in the dependent variable. A higher R² indicates that a larger proportion of the variability is explained by the model.\n",
    "- **R² = 1**: The model perfectly explains all the variability in the dependent variable, meaning that the predicted values match the actual values exactly.\n",
    "\n",
    "However, it's important to note that R² has its limitations. It does not indicate whether the regression coefficients are statistically significant, nor does it determine causation. A high R² doesn't necessarily mean that the model is a good fit, as overfitting can also result in a high R². Therefore, it's essential to consider R² alongside other model evaluation techniques and domain knowledge.\n",
    "\n",
    "In summary, R² gives you an idea of how well your linear regression model fits the data, with higher values indicating a better fit, but it should be used in conjunction with other evaluation methods to get a comprehensive understanding of your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff8eec",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "\n",
    "\n",
    "**Adjusted R-squared** is a modified version of the regular R-squared that takes into account the number of independent variables in a regression model. While the regular R-squared tells you how well your model fits the data, the adjusted R-squared provides a more nuanced view by considering both model fit and the complexity of the model due to the number of predictors.\n",
    "\n",
    "The formula for adjusted R-squared is:\n",
    "\n",
    "$ \\text{Adjusted } R^2 = 1 - \\frac{(1 - R^2) \\times (n - 1)}{n - k - 1} $\n",
    "\n",
    "Where:\n",
    "- $ R^2 $ is the regular R-squared.\n",
    "- $ n $ is the number of observations (data points).\n",
    "- $ k $ is the number of independent variables (predictors) in the model.\n",
    "\n",
    "The main difference between regular R-squared and adjusted R-squared is that the adjusted R-squared penalizes the inclusion of unnecessary variables in the model. As you add more predictors to a model, the regular R-squared will generally increase, even if the added predictors are not meaningful. This can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    "The adjusted R-squared addresses this issue by introducing a penalty term that accounts for the number of predictors. As you add more predictors, the penalty term increases, which reduces the adjusted R-squared if the added predictors do not contribute significantly to explaining the variation in the dependent variable. This helps to provide a more balanced evaluation of the model's complexity and fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565b1de",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Adjusted R-squared is more appropriate to use when you are comparing or evaluating regression models that have different numbers of predictors (independent variables). It helps mitigate the potential drawbacks of using the regular R-squared, which can sometimes lead to favoring overly complex models with a higher number of predictors.\n",
    "\n",
    "Here are some scenarios where adjusted R-squared is particularly useful:\n",
    "\n",
    "1. **Model Comparison**: When you are comparing multiple regression models with varying numbers of predictors, the adjusted R-squared can provide a fairer assessment of model performance. It accounts for both the goodness of fit and the complexity introduced by additional predictors.\n",
    "\n",
    "2. **Feature Selection**: Adjusted R-squared can guide feature selection by penalizing the inclusion of unnecessary variables. It helps prevent overfitting by discouraging the addition of predictors that don't contribute much to explaining the variability in the dependent variable.\n",
    "\n",
    "3. **Avoiding Overfitting**: Overfitting occurs when a model fits the training data too closely and performs poorly on new, unseen data. Adjusted R-squared helps address this issue by discouraging the inclusion of too many predictors, which can lead to overfitting.\n",
    "\n",
    "4. **Interpreting Model Fit**: In situations where you have a large number of predictors, a high regular R-squared might be misleading. The adjusted R-squared provides a more conservative measure of model fit, giving a clearer indication of how well the model is likely to generalize.\n",
    "\n",
    "5. **Complex Models**: When dealing with complex models that include numerous predictors, interactions, and terms, the adjusted R-squared helps you assess whether the additional complexity is justified in terms of improving model performance.\n",
    "\n",
    "6. **Communicating Results**: When presenting or explaining your model to others, adjusted R-squared can provide a more nuanced and accurate picture of how well your model fits the data, especially when the model is complex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555142f4",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "**RMSE (Root Mean Square Error)**, **MSE (Mean Squared Error)**, and **MAE (Mean Absolute Error)** are commonly used metrics in regression analysis to evaluate the performance of predictive models, especially in the context of continuous target variables.\n",
    "\n",
    "1. **RMSE (Root Mean Square Error)**:\n",
    "   RMSE is a measure of the average deviation between the predicted values and the actual values. It calculates the square root of the average of the squared differences between each predicted value and its corresponding actual value. RMSE gives more weight to larger errors, making it sensitive to outliers.\n",
    "\n",
    "   Mathematically, RMSE is calculated as:\n",
    "   \n",
    "   $ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $\n",
    "   \n",
    "   Where:\n",
    "   -  n  is the number of data points.\n",
    "   - $ y_i $ is the actual value for data point  i \n",
    "   - $ \\hat{y}_i $ is the predicted value for data point  i \n",
    "\n",
    "2. **MSE (Mean Squared Error)**:\n",
    "   MSE is a metric that calculates the average of the squared differences between predicted and actual values. It measures the average squared distance between the predicted and actual values, giving a sense of the overall magnitude of errors.\n",
    "\n",
    "   Mathematically, MSE is calculated as:\n",
    "   \n",
    "   $ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "   \n",
    "3. **MAE (Mean Absolute Error)**:\n",
    "   MAE is a metric that calculates the average of the absolute differences between predicted and actual values. It provides a measure of the average absolute magnitude of errors, regardless of direction.\n",
    "\n",
    "   Mathematically, MAE is calculated as:\n",
    "   \n",
    "   $ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $\n",
    "\n",
    "Where:\n",
    "- $ n $ is the number of data points.\n",
    "- $ y_i $ is the actual value for data point  i \n",
    "- $ \\hat{y}_i $ is the predicted value for data point  i \n",
    "\n",
    "In summary:\n",
    "- **RMSE** emphasizes larger errors due to the squaring of differences, and it's commonly used when you want to penalize larger errors more.\n",
    "- **MSE** is similar to RMSE but without the square root. It gives you a sense of the overall magnitude of errors.\n",
    "- **MAE** focuses on the absolute magnitude of errors and treats all errors equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d3bdd1",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "\n",
    "**Advantages of RMSE (Root Mean Square Error):**\n",
    "1. **Sensitivity to Large Errors**: RMSE puts more weight on larger errors due to squaring the differences. This can be beneficial if you want to emphasize and penalize significant deviations from the actual values.\n",
    "\n",
    "2. **Useful for Certain Applications**: In some domains, like finance or engineering, large errors might have more significant consequences. RMSE could be a suitable choice when modeling such situations.\n",
    "\n",
    "**Disadvantages of RMSE:**\n",
    "1. **Sensitivity to Outliers**: Because RMSE squares the errors, it's highly sensitive to outliers. Outliers can disproportionately influence the metric, leading to potentially misleading evaluations of model performance.\n",
    "\n",
    "2. **Lack of Intuitive Interpretation**: The squared nature of RMSE makes its units the square of the original units, which can be challenging to interpret intuitively.\n",
    "\n",
    "**Advantages of MSE (Mean Squared Error):**\n",
    "1. **Measuring Overall Error Magnitude**: MSE gives a clear sense of the overall magnitude of errors. It's useful for understanding how well the model is performing in terms of minimizing the sum of squared errors.\n",
    "\n",
    "2. **Useful for Optimization**: MSE is often used in optimization tasks where the goal is to minimize error magnitudes. Its mathematical properties make it well-suited for optimization algorithms.\n",
    "\n",
    "**Disadvantages of MSE:**\n",
    "1. **Units are Squared**: Similar to RMSE, MSE's units are the square of the original units, making interpretation less intuitive.\n",
    "\n",
    "2. **Sensitivity to Outliers**: Like RMSE, MSE is sensitive to outliers due to squaring the errors. Outliers can disproportionately impact the metric.\n",
    "\n",
    "**Advantages of MAE (Mean Absolute Error):**\n",
    "1. **Robustness to Outliers**: MAE is more robust to outliers since it only considers the absolute magnitude of errors. Outliers have a less pronounced effect on MAE compared to RMSE and MSE.\n",
    "\n",
    "2. **Intuitive Interpretation**: MAE's units are in the original scale of the data, making it easy to interpret. It represents the average absolute magnitude of errors.\n",
    "\n",
    "3. **Balancing Positive and Negative Errors**: MAE treats positive and negative errors equally, which can be desirable in situations where overestimations and underestimations have similar impacts.\n",
    "\n",
    "**Disadvantages of MAE:**\n",
    "1. **Less Sensitivity to Larger Errors**: Because MAE treats all errors equally, it might not adequately capture the impact of larger errors, especially if you need to emphasize and penalize them.\n",
    "\n",
    "2. **Less Sensitive to Improvements**: MAE might not be as sensitive to improvements in model performance compared to RMSE or MSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59840ee",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "**Lasso regularization**, also known as L1 regularization, is a technique used in regression analysis to prevent overfitting and improve the generalization performance of models. It achieves this by adding a penalty term to the linear regression cost function, encouraging the coefficients of less important features to be exactly zero. This results in feature selection, as some coefficients are effectively \"shrunk\" to zero, effectively excluding those features from the model.\n",
    "\n",
    "Here's how Lasso regularization works:\n",
    "\n",
    "In standard linear regression, the goal is to minimize the residual sum of squares (RSS):\n",
    "\n",
    "$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "In Lasso regularization, a penalty term is added to the RSS:\n",
    "\n",
    "$ \\text{Lasso Cost Function} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j| $\n",
    "\n",
    "Where:\n",
    "-  p  is the number of features (predictors).\n",
    "- $ \\beta_j $ is the coefficient of the  j -th feature.\n",
    "- $ \\lambda $ is the regularization parameter that controls the strength of the penalty.\n",
    "\n",
    "The key difference between Lasso regularization and Ridge regularization (L2 regularization) lies in the penalty term. While Lasso uses the absolute values of the coefficients, Ridge uses the squared values:\n",
    "\n",
    "$ \\text{Ridge Cost Function} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2 $\n",
    "\n",
    "**Differences between Lasso and Ridge Regularization:**\n",
    "1. **Feature Selection**: Lasso tends to push the coefficients of less important features to exactly zero, effectively performing feature selection. Ridge can shrink coefficients significantly but rarely makes them exactly zero.\n",
    "\n",
    "2. **Sparse Solutions**: Lasso produces sparse solutions, meaning it leads to models where only a subset of features have non-zero coefficients. Ridge doesn't result in fully sparse solutions.\n",
    "\n",
    "3. **Geometric Interpretation**: The constraint regions for Lasso and Ridge are different geometric shapes. The Lasso constraint forms a diamond shape, which can lead to corner solutions where coefficients are exactly zero.\n",
    "\n",
    "4. **Appropriate for Multicollinearity**: Ridge regularization handles multicollinearity (correlation between predictors) better than Lasso. Lasso might arbitrarily select one predictor over another in the presence of high multicollinearity.\n",
    "\n",
    "**When to Use Lasso Regularization:**\n",
    "Lasso regularization is more appropriate in scenarios where feature selection is desired, and it's believed that many of the features might not contribute significantly to the prediction. Use Lasso when:\n",
    "- You suspect that some features are irrelevant or redundant.\n",
    "- You want a simpler and more interpretable model.\n",
    "- You have a large number of features and want to reduce model complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ebd1a",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "Regularized linear models, such as Lasso and Ridge regression, help prevent overfitting in machine learning by introducing a penalty term to the cost function that discourages the model from fitting the training data too closely. This penalty encourages the model to generalize better to new, unseen data, by controlling the complexity of the model.\n",
    "\n",
    "Let's go through an example to illustrate how regularized linear models prevent overfitting:\n",
    "\n",
    "**Example: Polynomial Regression Overfitting**\n",
    "\n",
    "Suppose you have a dataset of housing prices based on the size of the house (in square feet). You want to build a polynomial regression model to predict the prices. You decide to use a high-degree polynomial (e.g., degree 20) to closely fit the training data.\n",
    "\n",
    "Here's what happens:\n",
    "\n",
    "1. **High-Degree Polynomial Fit (No Regularization)**:\n",
    "   You fit a degree-20 polynomial to the training data. The model has numerous coefficients, each contributing to a specific term in the polynomial equation. Since the model is extremely flexible, it can fit the training data almost perfectly, capturing even the smallest fluctuations in the data. However, this high flexibility can lead to overfitting, where the model learns the noise in the training data and fails to generalize to new data.\n",
    "\n",
    "2. **Regularized Polynomial Fit (with Ridge or Lasso)**:\n",
    "   Instead of fitting a degree-20 polynomial without any constraints, you apply Lasso or Ridge regularization. These techniques introduce a penalty term that discourages the model from assigning large values to the coefficients. As a result, the model is forced to strike a balance between fitting the training data and keeping the coefficients small.\n",
    "\n",
    "   - **Lasso**: Lasso may lead to some coefficients being exactly zero, effectively performing feature selection. This means the model selects the most important features and discards less relevant ones.\n",
    "   - **Ridge**: Ridge shrinks the coefficients toward zero but rarely makes them exactly zero. It handles multicollinearity better and prevents coefficients from becoming too large.\n",
    "\n",
    "**Outcome**:\n",
    "Regularization prevents the high-degree polynomial from fitting the noise in the training data. It smooths out the model's predictions, making it less prone to capturing random fluctuations. As a result, the regularized model is likely to generalize better to new data and avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fccdf",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "Regularized linear models, such as Lasso and Ridge regression, offer valuable tools for preventing overfitting and improving the generalization performance of models. However, they do have limitations and may not always be the best choice for every regression analysis. Here are some limitations to consider:\n",
    "\n",
    "1. **Loss of Interpretability**:\n",
    "   Regularization methods can lead to coefficients being shrunk towards zero or becoming exactly zero (in the case of Lasso). While this aids in feature selection and reduces overfitting, it can make it challenging to interpret the importance of individual features in the model.\n",
    "\n",
    "2. **Bias-Variance Trade-off**:\n",
    "   Regularization introduces a bias into the model to prevent overfitting. While this can lead to better generalization, it can also lead to underfitting if the regularization is too strong. Striking the right balance between bias and variance is essential, and sometimes simpler models without regularization might perform better.\n",
    "\n",
    "3. **Hyperparameter Tuning**:\n",
    "   Regularization models have hyperparameters, such as the strength of the penalty $ \\lambda $ (in Lasso and Ridge), that need to be tuned. Selecting the optimal value requires experimentation and validation, which can be time-consuming and may introduce bias if not done carefully.\n",
    "\n",
    "4. **Assumption of Linearity**:\n",
    "   Regularized linear models assume a linear relationship between the features and the target variable. If the true relationship is significantly nonlinear, regularized linear models may not capture the underlying patterns effectively.\n",
    "\n",
    "5. **Feature Scaling Importance**:\n",
    "   Regularization techniques are sensitive to the scale of features. It's important to scale features appropriately before applying these methods to ensure fair treatment of all features.\n",
    "\n",
    "6. **Multicollinearity Handling**:\n",
    "   While Ridge regression can handle multicollinearity between predictors quite well, Lasso can arbitrarily choose one of the correlated features over another, making it less reliable in such situations.\n",
    "\n",
    "7. **Data Size and Sparsity**:\n",
    "   Regularization methods might not work well with very small datasets, as there might not be sufficient information to estimate the regularization parameter accurately. Additionally, when dealing with high-dimensional sparse data, Lasso might struggle to select the right features.\n",
    "\n",
    "8. **Alternative Techniques**:\n",
    "   Depending on the problem, other techniques like decision trees, random forests, support vector machines, or neural networks might perform better without the assumptions and constraints imposed by linear models.\n",
    "\n",
    "9. **Model Complexity Control**:\n",
    "   While regularization controls model complexity, it might not address all complexities, such as interactions between variables, which can be essential in some cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d76163",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "In this scenario, you are comparing two regression models, Model A and Model B, using different evaluation metrics: RMSE (Root Mean Square Error) for Model A and MAE (Mean Absolute Error) for Model B.\n",
    "\n",
    "**RMSE for Model A: 10**\n",
    "**MAE for Model B: 8**\n",
    "\n",
    "When choosing between the two models, you need to consider the specific goals of your analysis and the characteristics of the problem. Both RMSE and MAE have their advantages and limitations.\n",
    "\n",
    "**Choosing Based on RMSE (Model A):**\n",
    "RMSE emphasizes larger errors due to the squaring of differences. It's sensitive to outliers and gives more weight to larger deviations. If minimizing large errors is crucial and outliers have significant impact on your application (e.g., in finance or safety-critical systems), Model A might be preferred due to its lower RMSE.\n",
    "\n",
    "**Choosing Based on MAE (Model B):**\n",
    "MAE considers the absolute magnitude of errors without squaring them. It treats all errors equally and is less sensitive to outliers. If a balanced assessment of overall model performance is important and you want to minimize the average absolute error, Model B might be favored due to its lower MAE.\n",
    "\n",
    "**Limitations and Considerations:**\n",
    "1. **Outliers**: If the dataset contains outliers that disproportionately affect the RMSE, it could skew the evaluation in favor of Model B with the lower MAE.\n",
    "\n",
    "2. **Balance**: While RMSE focuses more on larger errors, MAE considers all errors equally. The choice between the two depends on your application's tolerance for different magnitudes of errors.\n",
    "\n",
    "3. **Units**: RMSE's units are the square of the original units, while MAE's units are the same as the original units. Depending on the context, one might be easier to interpret than the other.\n",
    "\n",
    "4. **Model Complexity**: These metrics don't provide insights into model complexity or whether a model is overfitting or underfitting. You should also consider model complexity, feature importance, and other evaluation metrics if available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7c52e",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "\n",
    "Choosing between Ridge and Lasso regularization depends on the specific characteristics of your data and the goals of your analysis. Let's analyze the situation with the given regularization parameters:\n",
    "\n",
    "**Model A (Ridge Regularization, λ = 0.1):**\n",
    "Ridge regularization adds a penalty term proportional to the square of the coefficients. A smaller value of λ (0.1 in this case) indicates a relatively weaker penalty, allowing the coefficients to be moderately large.\n",
    "\n",
    "**Model B (Lasso Regularization, λ = 0.5):**\n",
    "Lasso regularization adds a penalty term proportional to the absolute value of the coefficients. A larger value of λ (0.5 in this case) implies a stronger penalty, encouraging more coefficients to be exactly zero.\n",
    "\n",
    "**Choosing the Better Model:**\n",
    "In this scenario, it's not immediately clear which model is better without more context. The choice between Ridge and Lasso regularization involves trade-offs and depends on the characteristics of your data:\n",
    "\n",
    "**Ridge Regularization (Model A):**\n",
    "- Ridge tends to shrink coefficients towards zero without making them exactly zero.\n",
    "- It's better suited when you believe that most features are relevant, but some regularization is needed to prevent overfitting.\n",
    "- Ridge can handle multicollinearity well and can be more stable when dealing with correlated predictors.\n",
    "- It's generally suitable when you have a moderate amount of features and want to balance between feature selection and keeping all features.\n",
    "\n",
    "**Lasso Regularization (Model B):**\n",
    "- Lasso tends to make some coefficients exactly zero, performing feature selection.\n",
    "- It's useful when you suspect that many features are irrelevant and should be eliminated.\n",
    "- Lasso might not perform well with high multicollinearity, as it arbitrarily selects one of the correlated predictors and discards the rest.\n",
    "- If the true model contains many small effects, Lasso's feature selection can lead to a more parsimonious model.\n",
    "\n",
    "**Trade-offs and Limitations:**\n",
    "- **Feature Selection vs. Coefficient Shrinkage**: The main trade-off between Ridge and Lasso is feature selection (Lasso) versus coefficient shrinkage (Ridge).\n",
    "- **Model Interpretability**: Lasso's feature selection might make the model more interpretable by focusing on a subset of important features.\n",
    "- **Hyperparameter Tuning**: The choice of λ is critical in both methods. The values of 0.1 and 0.5 used here might not be optimal for your specific problem, and fine-tuning is essential.\n",
    "- **Data Characteristics**: The effectiveness of each regularization method depends on your data's characteristics, such as the number of features, the amount of multicollinearity, and the true underlying model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
